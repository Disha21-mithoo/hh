# All general configurations will be under this header
general:
  output_directory: results  # The output directory to save the training progress and results
  experiment_id: test # The experiment id
  # The name of the model checkpoints directory to save the intermediate model checkpoints
  model_checkpoints_directory_name: checkpoints

# All configurations related to dataloader will be under this header
dataloader:
  name: cub_200_2011_contrastive  # Name of the dataloader to be used
  # Fraction of training data to be used for current training, it may by 0.1 or 0.2 for semi-supervised fine tuning
  train_data_fraction: 1
  test_data_fraction: 1  # Fraction of test data to be used during current training
  download: True  # Flag in order to decide if to download the dataset or not
  root_directory_path: ./data/CUB_200_2011  # The root directory path of the dataset
  resize_width: 200  # Image resize width
  resize_height: 200  # Image resize height
  batch_size: 8  # Batch size for training and testing
  shuffle: True  # Either to shuffle the dataset for training or not
  num_workers: 4  # Number of parallel workers to load the dataset
  # The train and test data transforms
  transforms:
    # Train transforms used during training
    train:
      t_1:
        path: torchvision.transforms.RandomCrop
        param:
          size: 112
      t_2:
        path: torchvision.transforms.RandomHorizontalFlip
      t_3:
        path: torchvision.transforms.ToTensor
      t_4:
        path: torchvision.transforms.Normalize
        param:
          mean: [0.485, 0.456, 0.406]
          std: [0.229, 0.224, 0.225]
    # Test transforms used during testing
    test:
      t_1:
        path: torchvision.transforms.RandomCrop
        param:
          size: 112
      t_2:
        path: torchvision.transforms.ToTensor
      t_3:
        path: torchvision.transforms.Normalize
        param:
          mean: [0.485, 0.456, 0.406]
          std: [0.229, 0.224, 0.225]
    contrastive: transformations.barlow_twins.Transform

# All configurations related to model will be under this header
model:
  name: torchvision_ssl_barlow_twins  # Name/source of the model
  # Complete model class path (i.e. torchvision.models.resnet50, torchvision.models.alexnet, etc.)
  model_function_path: torchvision.models.resnet50
  pretrained: True  # Either to load weights from pretrained imagenet model
  classes_count: 200  # Number of classes
  rotation_classes_count: 4  # Number of rotation classes
  # Path to load pre-trained weights from
#  checkpoints_path: /home/mm/Desktop/MBZUAI/Semester_1/ML701/Project/Post_MidTerm/Contrastive_Learning/ImageNet_Pretrained_Weights/vissl_converted/converted_pirl.torch
#  checkpoints_path: /home/mm/Desktop/MBZUAI/Semester_1/ML701/Project/Post_MidTerm/Contrastive_Learning/ImageNet_Pretrained_Weights/vissl_converted/converted_simclr.torch
#  checkpoints_path: /home/mm/Desktop/MBZUAI/Semester_1/ML701/Project/Post_MidTerm/Contrastive_Learning/ImageNet_Pretrained_Weights/vissl_converted/converted_swav.torch
#  checkpoints_path: /home/mm/Desktop/MBZUAI/Semester_1/ML701/Project/Post_MidTerm/Contrastive_Learning/ImageNet_Pretrained_Weights/vissl_converted/barlow_twins.pth
#  checkpoints_path: /home/mm/Desktop/MBZUAI/Semester_1/ML701/Project/Post_MidTerm/Contrastive_Learning/CUB_Pretrained_Weights_100/vissl_converted/converted_vissl_simclr_cub.torch
# Configuration related to the diversification block will be under this head
diversification_block:
  p_peak: 0.5
  p_patch: 0.5
  patch_size: 3  # Patch size to be suppressed
  alpha: 0.1  # Suppression factor

# All configurations related to training will be under this header
train:
  name: ssl_bt_trainer  # Name of the trainer to use
  epochs: 200  # Number of epochs
  warm_up_epochs: 10  # Number of warm up epochs
  warm_up_loss_function_path: torch.nn.CrossEntropyLoss  # Standard cross entropy loss
  class_loss_function_path: torch.nn.CrossEntropyLoss  # Loss function
  rotation_loss_function_path: torch.nn.CrossEntropyLoss  # Standard cross entropy loss for rotation prediction
  rotation_loss_weight: 0.2
  # Optimizer related configurations
  optimizer_path: torch.optim.SGD  # Complete optimizer class path
  optimizer_param:
    lr: 0.001  # Learning rate
    momentum: 0.9  # Momentum
    weight_decay: 0.0001  # Weight Decay
  # Learning rate scheduler configurations
  lr_scheduler:
    step_size: 60  # Step size
    gamma: 0.1  # Decay factor

